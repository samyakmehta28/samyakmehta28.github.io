<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Tranformer Encoder and Decoder Tasks | Samyak S. Mehta </title> <meta name="author" content="Samyak S. Mehta"> <meta name="description" content="This project involves developing a transformer encoder for politician speech prediction, training a GPT-like decoder for speech generation, and experimenting with architecture variations to enhance model accuracy and perplexity."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?b05f9a0b7405d7c8c89c7465593dea81"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?dadeb9c5d1fd12bc8d37475657446863"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://samyakmehta28.github.io/projects/6_project/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Samyak</span> S. Mehta </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Tranformer Encoder and Decoder Tasks</h1> <p class="post-description">This project involves developing a transformer encoder for politician speech prediction, training a GPT-like decoder for speech generation, and experimenting with architecture variations to enhance model accuracy and perplexity.</p> </header> <article> <p>This was my project for the course Statistical NLP, completed under the guidance of Professor Ndapa Nakashole, during the SP24 quarter of my graduate studies at the University of California, San Diego. The project report can be found <a href="https://github.com/samyakmehta28/CSE256_PA2/blob/master/CSE256_PA2_report.pdf" rel="external nofollow noopener" target="_blank">here</a> and the code can be found in the github repository <a href="https://github.com/samyakmehta28/CSE256_PA2" rel="external nofollow noopener" target="_blank">here</a>.</p> <h1 id="encoder-and-decoder-transformer-architecture">Encoder and Decoder Transformer Architecture</h1> <p>This project involves developing a transformer encoder for politician speech prediction and training a GPT-like decoder for speech generation. This project evaluates different positional encoding techniques and tokenization strategies in transformer models, focusing on their impact on language modeling and classification tasks. The study aims to improve model performance by experimenting with alternative architectures and hyper-parameter tuning.</p> <h3 id="introduction">Introduction:</h3> <p>Transformers have revolutionized natural language processing by effectively handling sequences. This project explores three positional encoding strategies: Absolute Positional Encoding, Non-Positional Embedding (NOPE), and AliBi Embedding. Additionally, it investigates the effect of varying tokenization methods on model performance. The goal is to enhance the model’s ability to handle tasks requiring a deep understanding of sequences and contextual relationships.</p> <h3 id="datasets">Datasets:</h3> <ul> <li>Language Modeling Task: Training and testing datasets consisting of speeches from different presidents, including George W. Bush, Barack Obama, and others.</li> <li>Classification Task: Datasets used for comparing the performance of models with different positional encodings on tasks such as sentiment analysis.</li> </ul> <h3 id="methodology">Methodology:</h3> <p>The study involved:</p> <ul> <li>Encoder and Decoder Models: Evaluating models with different positional encodings across multiple epochs.</li> <li>Tokenization Strategies: Comparing performance using default and custom tokenizers.</li> <li>Hyper-parameter Tuning: Fine-tuning parameters such as learning rates, number of layers, and tokenization methods to optimize model performance.</li> </ul> <h3 id="key-findings">Key Findings:</h3> <ul> <li> <p>Positional Encoding:</p> <ul> <li>AliBi: Enhanced long-range dependency understanding, outperforming traditional Absolute Positional Encoding in language modeling tasks.</li> <li>NOPE: Focused on token semantics, leading to better results in classification tasks.</li> </ul> </li> <li>Tokenization: Custom Tokenizer: Improved model accuracy and performance, achieving a final test accuracy of 86.67%.</li> <li>Hyper-parameter Tuning: Fine-tuning led to smoother training curves and better model generalization across different tasks.</li> </ul> <h3 id="analysis-and-categorization-of-errors">Analysis and Categorization of Errors:</h3> <ul> <li> <p>Errors were categorized into:</p> <ul> <li>Positional Misinterpretation: Challenges with long sequences in Absolute Positional Encoding.</li> <li>Tokenization Issues: Difficulties in handling diverse text structures due to suboptimal tokenization.</li> </ul> </li> </ul> <h3 id="conclusion">Conclusion:</h3> <p>The project demonstrates the significance of choosing appropriate positional encodings and tokenization methods in transformer models. The findings highlight the potential of alternative embeddings like AliBi and NOPE in improving model performance for specific tasks. The study provides valuable insights for future enhancements in natural language processing models.</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Samyak S. Mehta. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>